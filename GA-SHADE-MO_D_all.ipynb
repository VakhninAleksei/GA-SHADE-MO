{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9e0691",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "from statistics import mean\n",
    "import plotly.express as px\n",
    "from sklearn.svm import SVR\n",
    "import plotly.graph_objs as go\n",
    "from scipy.stats import cauchy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVR\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.linear_model import Ridge\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from scipy.spatial.distance import cdist\n",
    "from plotly.subplots import make_subplots\n",
    "from pytz import timezone as pytz_timezone\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a92c9",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_agreement (s, o):\n",
    "\n",
    "    s,o = filter_nan(s,o)\n",
    "    ia = 1 -(np.sum((o-s)**2))/(np.sum((np.abs(s-np.mean(o))+np.abs(o-np.mean(o)))**2))\n",
    "    return ia\n",
    "\n",
    "\n",
    "def filter_nan(s,o):\n",
    "\n",
    "    s = np.array(s.copy())\n",
    "    o = np.array(o.copy())\n",
    "    data = np.array([s.flatten(),o.flatten()])\n",
    "    data = np.transpose(data)\n",
    "    data = data[~np.isnan(data).any(1)]\n",
    "    s = data[:,0]\n",
    "    o = data[:,1]\n",
    "    return s, o\n",
    "\n",
    "    print(relationship.mean)\n",
    "\n",
    "\n",
    "def create_the_closest_set(pop_size, percent):\n",
    "    points = np.zeros((pop_size, 2))\n",
    "    step = 1/(pop_size-1)\n",
    "    for i in range (0,pop_size):\n",
    "        coef1 = step*i\n",
    "        coef2 = 1 - coef1\n",
    "        points[i][0] = coef1\n",
    "        points[i][1] = coef2\n",
    "\n",
    "\n",
    "    distance_matrix = cdist(points, points, 'euclidean')  \n",
    "\n",
    "    neighbors = [np.argsort(distance_matrix[i]) for i in range(len(points))]\n",
    "\n",
    "    for i, neighbor_indices in enumerate(neighbors):\n",
    "        \n",
    "        for neighbor_index in neighbor_indices:\n",
    "            if neighbor_index != i:\n",
    "                distance = distance_matrix[i, neighbor_index]\n",
    "    index_to = int(pop_size*percent/100)+1\n",
    "    df = pd.DataFrame(neighbors)\n",
    "    df = df.iloc[:, 1:index_to]\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_indices(pop_size, A, p, i, set_of_indeces_for_mutation):\n",
    "    index_set = set_of_indeces_for_mutation\n",
    "    \n",
    "    rnd_set = np.array(index_set.iloc[i])\n",
    "    np.random.shuffle(rnd_set)\n",
    "    print(rnd_set)\n",
    "    return rnd_set[0], rnd_set[1],rnd_set[2]\n",
    "\n",
    "def fitness_function(df, params, variable_type, n1, n2, MAE_norm, index_individual, pop_size, criterias, min_f1, max_f1, min_f2, max_f2):\n",
    "    FEATURES = ['Temp', 'Cloud', 'Pressure', 'Rel-hum', 'Dew-point', 'Wind_cos', 'Wind_sin', 'Wind_sp',\n",
    "                'Temp_lag_1', 'Temp_lag_2', 'Temp_lag_3', 'P_lag_1', 'P_lag_2', 'P_lag_3',\n",
    "                'd_cos', 'd_sin', 'w_cos', 'w_sin', 'm_cos', 'm_sin']\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    df = df[(df.index.year == 2015) | (df.index.year == 2016) | (df.index.year == 2017)]\n",
    "\n",
    "    X = df.drop(columns=['energy'])\n",
    "    y = df['energy']\n",
    "  \n",
    "    feature_variables = params[n1:]\n",
    "    new_features = []\n",
    "    number_of_ones = 0\n",
    "    validation_error = 0.0\n",
    "\n",
    "    for i in range(0, len(feature_variables)):\n",
    "        if feature_variables[i] == 1:\n",
    "            new_features += [FEATURES[i]]\n",
    "            number_of_ones += 1\n",
    "    \n",
    "    X = X[new_features]\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    for i in range(0, n1):\n",
    "        if variable_type[i] == 'int':\n",
    "            params[i] = int(params[i])\n",
    "\n",
    "    colsample_bytree = params[0]\n",
    "    learning_rate = params[1]\n",
    "    max_depth = int(params[2])\n",
    "    alpha = int(params[3])\n",
    "    n_estimators = int(params[4])\n",
    "\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=colsample_bytree,\n",
    "                             learning_rate=learning_rate, max_depth=max_depth, alpha=alpha, n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mae_scores = []\n",
    "    weights = [0.0667, 0.1333, 0.2, 0.2667, 0.3333]  \n",
    "\n",
    "    for fold_idx, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        weighted_mae = weights[fold_idx] * mae\n",
    "        mae_scores.append(weighted_mae)\n",
    "\n",
    "    performance = np.sum(mae_scores) \n",
    "    f1 = (performance - min_f1) / (max_f1 - min_f1)\n",
    "    f2 = (number_of_ones - min_f2) / (max_f2 - min_f2)\n",
    "    step = 1.0 / (pop_size - 1)\n",
    "    coeff_1 = step * index_individual\n",
    "    coeff_2 = 1.0 - coeff_1\n",
    "\n",
    "    validation_error += coeff_1 * f1 + coeff_2 * f2\n",
    "\n",
    "    if criterias == 0:\n",
    "        return validation_error\n",
    "    if criterias == 1:\n",
    "        return f1, f2\n",
    "    if criterias == 2:\n",
    "        return performance\n",
    "\n",
    "\n",
    "def init_population(population, pop_size, N, a, b, variable_type):\n",
    "    for i in range(0, pop_size):\n",
    "        for j in range (0, N):\n",
    "            if (variable_type[j] == 'real'):\n",
    "                population[i][j] = random.random()*(b[j]-a[j])+a[j]\n",
    "            if (variable_type[j] == 'int'):\n",
    "                population[i][j] = int(random.random()*(b[j]-a[j])+a[j])\n",
    "            if (variable_type[j] == 'bool'):\n",
    "                if (random.random()>0.5):\n",
    "                    population[i][j] = 1\n",
    "                else:\n",
    "                    population[i][j] = 0\n",
    "\n",
    "def generate_indices(pop_size, A, p, i, set_of_indeces_for_mutation):\n",
    "    index_set = set_of_indeces_for_mutation\n",
    "    \n",
    "    rnd_set = np.array(index_set.iloc[i])\n",
    "    np.random.shuffle(rnd_set)\n",
    "    return rnd_set[0], rnd_set[1],rnd_set[2]\n",
    "\n",
    "def borders (v, population, N, a, b, variable_type):\n",
    "    for i in range(0, pop_size):\n",
    "        for j in range (0, N):\n",
    "            if (variable_type[j] == 'real'):\n",
    "                if (v[i][j] > b[j]):\n",
    "                    v[i][j] = (b[j]+population[i][j])/2\n",
    "                if (v[i][j] < a[j]):\n",
    "                    v[i][j] = (a[j]+population[i][j])/2\n",
    "            if (variable_type[j] == 'int'):\n",
    "                if (v[i][j] > b[j]):\n",
    "                    v[i][j] = int((b[j]+population[i][j])/2)\n",
    "                if (v[i][j] < a[j]):\n",
    "                    v[i][j] = int((a[j]+population[i][j])/2)\n",
    "def isNaN(num):\n",
    "    return num != num\n",
    "\n",
    "def calc_criterions (df, params,n1,n2):\n",
    "    global_train_MAE = 0.0\n",
    "    global_train_MSE = 0.0\n",
    "    global_train_IA = 0.0\n",
    "    global_train_R2 = 0.0\n",
    "    \n",
    "    global_val_MAE = 0.0\n",
    "    global_val_MSE = 0.0\n",
    "    global_val_IA = 0.0\n",
    "    global_val_R2 = 0.0\n",
    "    \n",
    "    global_test_MAE = 0.0\n",
    "    global_test_MSE = 0.0\n",
    "    global_test_IA = 0.0\n",
    "    global_test_R2 = 0.0\n",
    "    \n",
    "    data = df\n",
    "    FEATURES = ['Temp','Cloud', 'Pressure','Rel-hum','Dew-point','Wind_cos','Wind_sin','Wind_sp',\n",
    "          'Temp_lag_1','Temp_lag_2','Temp_lag_3','P_lag_1', 'P_lag_2','P_lag_3',\n",
    "         'd_cos','d_sin','w_cos','w_sin','m_cos','m_sin']\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    df = df[(df.index.year==2015) | (df.index.year==2016)| (df.index.year==2017)]\n",
    "\n",
    "    X = df.drop(columns=['energy'])\n",
    "    y = df['energy']\n",
    "  \n",
    "   \n",
    "    feature_variables = params[n1:]\n",
    "    new_features = []\n",
    "    number_of_ones = 0\n",
    "    validation_error = 0.0\n",
    "    for i in range(0, len(feature_variables)):\n",
    "        if (feature_variables[i] == 1):\n",
    "            new_features += [FEATURES[i]]\n",
    "            number_of_ones = number_of_ones + 1\n",
    "    \n",
    "    X = X[new_features] \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    for i in range(0, n1):\n",
    "        if (variable_type[i] == 'int'):\n",
    "            params[i] = int(params[i])\n",
    "\n",
    "    colsample_bytree =params[0]\n",
    "    learning_rate = params[1]\n",
    "    max_depth = int(params[2])\n",
    "    alpha = int(params[3])\n",
    "    n_estimators = int(params[4])\n",
    "\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=colsample_bytree,\n",
    "                              learning_rate=learning_rate, max_depth=max_depth, alpha=alpha, n_estimators=n_estimators)\n",
    "    \n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    mae_scores_train = []\n",
    "    mse_scores_train = []\n",
    "    r2_scores_train = []\n",
    "    ia_scores_train = []\n",
    "\n",
    "    mae_scores_val = []\n",
    "    mse_scores_val = []\n",
    "    r2_scores_val = []\n",
    "    ia_scores_val = []\n",
    "\n",
    "    mae_scores_test = []\n",
    "    mse_scores_test = []\n",
    "    r2_scores_test = []\n",
    "    ia_scores_test = []\n",
    "\n",
    "    weights = [0.0667, 0.1333, 0.2, 0.2667, 0.3333]      \n",
    "    for fold_idx, (train_index, val_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        \n",
    "        X_val = scaler.transform(X_val)\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "                \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_pred = model.predict(X_train)\n",
    "\n",
    "        mae = mean_absolute_error(y_train, y_pred) * weights[fold_idx]\n",
    "        \n",
    "        mse = mean_squared_error(y_train, y_pred) * weights[fold_idx]\n",
    "        r2 = r2_score(y_train, y_pred) * weights[fold_idx]\n",
    "        ia = index_of_agreement(y_train, y_pred) * weights[fold_idx]\n",
    "\n",
    "        mae_scores_train.append(mae)\n",
    "        mse_scores_train.append(mse)\n",
    "        r2_scores_train.append(r2) \n",
    "        ia_scores_train.append(ia) \n",
    "\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        weighted_mae = weights[fold_idx] * mae\n",
    "        \n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        weighted_mse = weights[fold_idx] * mse\n",
    "        \n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        weighted_r2 = weights[fold_idx] * r2\n",
    "        \n",
    "        ia = index_of_agreement(y_val, y_pred)\n",
    "        weighted_ia = weights[fold_idx] * ia\n",
    "        \n",
    "\n",
    "        mae_scores_val.append(weighted_mae)\n",
    "        mse_scores_val.append(weighted_mse)\n",
    "        r2_scores_val.append(weighted_r2) \n",
    "        ia_scores_val.append(weighted_ia)\n",
    "\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    df = data\n",
    "\n",
    "    df_train = df[(df.index.year==2015) | (df.index.year==2016)| (df.index.year==2017)]\n",
    "    df_test = df[(df.index.year==2018)]\n",
    "\n",
    "    X_train = df_train.drop(columns=['energy'])\n",
    "    X_train = X_train[new_features]\n",
    "    \n",
    "    y_train = df_train['energy']\n",
    "    X_test = df_test.drop(columns=['energy'])\n",
    "    X_test = X_test[new_features]\n",
    "    y_test = df_test['energy']\n",
    "    \n",
    "    \n",
    "        \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    " \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    global_train_MAE = np.sum(mae_scores_train)\n",
    "    global_train_MSE =np.sum(mse_scores_train)\n",
    "    global_train_R2 = np.sum(r2_scores_train)\n",
    "    global_train_IA = np.sum(ia_scores_train)\n",
    "\n",
    "    global_val_MAE = np.sum(mae_scores_val)\n",
    "    global_val_MSE = np.sum(mse_scores_val)\n",
    "    global_val_R2 = np.sum(r2_scores_val)\n",
    "    global_val_IA = np.sum(ia_scores_val)\n",
    "\n",
    "        \n",
    "    global_test_MAE = mean_absolute_error(y_test, y_pred)\n",
    "    global_test_MSE = mean_squared_error(y_test, y_pred)\n",
    "    global_test_R2 = r2_score(y_test, y_pred)\n",
    "    global_test_IA = index_of_agreement(y_test, y_pred)\n",
    "        \n",
    "    \n",
    "    return global_train_MAE, global_train_MSE, global_train_R2, global_train_IA, global_val_MAE,global_val_MSE,global_val_R2,global_val_IA,global_test_MAE,global_test_MSE,global_test_R2,global_test_IA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a7902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pop_size = 100\n",
    "n1 = 5\n",
    "n2 = 20\n",
    "N = n1 + n2\n",
    "pop_size = 100\n",
    "\n",
    "df = pd.read_csv(\"data_daily.csv\", index_col=0)\n",
    "set_of_indeces_for_mutation = create_the_closest_set(pop_size, 10)\n",
    "\n",
    "\n",
    "RUNS = 5\n",
    "columns = [\"colsample_bytree\",\"learning_rate\", \"max_depth\", \"alpha\",\"n_estimators\",\n",
    "        'Temp','Cloud', 'Pressure','Rel-hum','Dew-point','Wind_cos','Wind_sin','Wind_sp',\n",
    "          'Temp_lag_1','Temp_lag_2','Temp_lag_3','P_lag_1', 'P_lag_2','P_lag_3',\n",
    "         'd_cos','d_sin','w_cos','w_sin','m_cos','m_sin']\n",
    "df_stats = pd.DataFrame()\n",
    "\n",
    "variable_type =  ['real','real','int','int','int',\n",
    "                  'bool','bool','bool','bool','bool','bool','bool','bool','bool','bool',\n",
    "                  'bool','bool','bool','bool','bool','bool','bool','bool','bool','bool',\n",
    "                  ]\n",
    "a =[0.001, 0.001,1,1,1]\n",
    "b = [1,   1,   20,10,300]\n",
    "\n",
    "H = 10\n",
    "\n",
    "best_in_RUN = []\n",
    "fitness_stat = pd.DataFrame()\n",
    "solution_stat = []\n",
    "\n",
    "while (RUNS>0):\n",
    "\n",
    "    global_best = 1e300\n",
    "    population = np.empty((pop_size,N))\n",
    "    v = np.empty((pop_size,N))\n",
    "\n",
    "    fitness = np.empty(pop_size)\n",
    "    fitness_new = np.empty(pop_size)\n",
    "    F_history = np.empty(H)\n",
    "    CR_history = np.empty(H)\n",
    "    S_CR = []\n",
    "    S_F = []\n",
    "    w = []\n",
    "    r = np.empty(pop_size)\n",
    "    CR = np.empty(pop_size)\n",
    "    F = np.empty(pop_size)\n",
    "    p = np.empty(pop_size)\n",
    "\n",
    "    FEV = 5000\n",
    "    kratnost = FEV/100\n",
    "    tracking_fitness = []\n",
    "    tracking_solution = []\n",
    "    init_population(population, pop_size, N, a, b, variable_type)\n",
    "    MAE_norm = 0\n",
    "    find_max_mae = []\n",
    "    \n",
    "    for i in range (0, pop_size):\n",
    "        if (int(sum(population[i][n1:])) == 0):\n",
    "            population[i][n1:n1+random.randint(0, n2)]=1\n",
    "        print(i, ' ')\n",
    "        x = population[i][:]\n",
    "        solution = x.copy()\n",
    "        #print(i, \" \", solution)\n",
    "        mae = calc_criterions (df, x,n1,n2)[4]\n",
    "        find_max_mae.append(mae)   \n",
    "        print(find_max_mae)\n",
    "        print(np.max(find_max_mae))\n",
    "        \n",
    "    f1_max = np.max(find_max_mae)\n",
    "    f1_min = 0.0\n",
    "    f2_max = n2\n",
    "    f2_min = 1\n",
    "    \n",
    "    \n",
    "    for i in range (0, pop_size):\n",
    "        if (int(sum(population[i][n1:])) == 0):\n",
    "            population[i][n1:n1+random.randint(0, n2)]=1\n",
    "\n",
    "        x = population[i][:]\n",
    "        solution = x.copy()\n",
    "        \n",
    "        fit = fitness_function(df, x, variable_type ,n1, n2, 2.5, i, pop_size,0, min_f1,max_f1,min_f2,max_f2)\n",
    "        fitness[i] = fit\n",
    "    \n",
    "\n",
    "        fitness_new[i] = fitness[i]\n",
    "        FEV =FEV - 1\n",
    "\n",
    "        if (fitness[i]<global_best):\n",
    "            global_best = fitness[i]\n",
    "            best_solution = solution.copy()\n",
    "            print(\"GLOVAL BEST, FEV: \", FEV,\": \", global_best, \", best solution: \", best_solution )\n",
    "\n",
    "        if (FEV % kratnost == 0):\n",
    "            tracking_fitness.append(global_best)\n",
    "            tracking_solution = (best_solution)\n",
    "\n",
    "    F_history[:] = 0.5\n",
    "    CR_history[:] = 0.5\n",
    "    A = 0\n",
    "    k=0\n",
    "    pmin = 5.0/pop_size\n",
    "    MAE_norm = min(fitness)\n",
    "    print(MAE_norm)\n",
    "    while (FEV>0):\n",
    "        CR_df = pd.DataFrame(pd.DataFrame(CR_history).T)\n",
    "\n",
    "        S_CR = np.empty(pop_size)\n",
    "        S_F = np.empty(pop_size)\n",
    "        v[:][:]=population[:][:]\n",
    "        for i in range (0, pop_size):\n",
    "            r[i] = int(random.random()*H)\n",
    "            CR[i] = np.random.normal(CR_history[int(r[i])], 0.1)\n",
    "            if CR[i]>1:\n",
    "                CR[i] = 1\n",
    "            if CR[i]<0:\n",
    "                CR[i] = 0\n",
    "\n",
    "            F[i] = F_history[int(r[i])]+np.random.standard_cauchy()*0.1\n",
    "            if F[i]>1:\n",
    "                F[i] = 1\n",
    "            while F[i]<0:\n",
    "                F[i] = F_history[int(r[i])]+np.random.standard_cauchy()*0.1\n",
    "            p[i] = random.random()*(0.2-pmin)+pmin\n",
    "\n",
    "            r1, r2, r3 = generate_indices(pop_size, A, p, i, set_of_indeces_for_mutation)\n",
    "\n",
    "            j_rand = int(random.random()*N)\n",
    "            for j in range (0,n1):\n",
    "                if (random.random()<CR[i] or j == j_rand):\n",
    "                    v[i][j] =population[i][j]+F[i]*(population[r3][j]-population[i][j])+F[i]*(population[r1][j]-population[r2][j])\n",
    "\n",
    "\n",
    "            for j in range (n1,n1 + n2):\n",
    "\n",
    "                if (random.random()<CR[i] or j == j_rand):\n",
    "                    rnd_bool_var = pop_size+1\n",
    "                    var_list = [i,r1, r2, r3]\n",
    "                    while (rnd_bool_var >= pop_size):  \n",
    "                        rnd_bool_var = random.choice(var_list)\n",
    "\n",
    "                    v[i][j] = population[rnd_bool_var][j]\n",
    "\n",
    "\n",
    "            if (random.random()<(1.0/(n2))):\n",
    "                rnd_ind =random.randint(n1, n1 + n2-1) \n",
    "                if (v[i][rnd_ind] == 0):\n",
    "                    v[i][rnd_ind] =1\n",
    "                else:\n",
    "                     v[i][rnd_ind] =1\n",
    "\n",
    "            for j in range (0,n1):\n",
    "                if ( (isNaN(v[i][j]) == 1)):\n",
    "                    if variable_type[j] == 'real':\n",
    "                        v[i][j] = random.uniform(a[j], b[j])\n",
    "                    if variable_type[j] == 'int':\n",
    "                        v[i][j] = random.randint(int(a[j]), int(b[j]))\n",
    "\n",
    "            chech_sum_ones = 0\n",
    "            for j in range (n1,n1 + n2):\n",
    "                if (v[i][j] == 1):\n",
    "                    chech_sum_ones = chech_sum_ones +1\n",
    "            if (chech_sum_ones == 0):\n",
    "                v[i][random.randint(n1, n1 + n2-1)] = 1\n",
    "            borders (v, population, N, a, b, variable_type)\n",
    "\n",
    "\n",
    "\n",
    "        w = np.array([])\n",
    "        S_CR = np.array([])\n",
    "        S_F = np.array([])\n",
    "        for i in range (0, pop_size):\n",
    "\n",
    "            solution = v[i][:]\n",
    "            x = solution = v[i][:]\n",
    "            fit = fitness_function(df, x, variable_type ,n1, n2, 2.5, i, pop_size,0, min_f1,max_f1,min_f2,max_f2)\n",
    "            fitness_new[i] = fit\n",
    "            FEV = FEV-1\n",
    "            if (fitness_new[i]<fitness[i]):\n",
    "                S_CR = np.append(S_CR, CR[i])\n",
    "                S_F = np.append(S_F, F[i])\n",
    "                w = np.append(w,(fitness[i]-fitness_new[i]))\n",
    "\n",
    "                population[i][:] = solution\n",
    "\n",
    "                fitness[i] =fitness_new[i]\n",
    "\n",
    "                if (fitness[i]<global_best):\n",
    "                    global_best = fitness[i]\n",
    "                    best_solution = solution.copy()\n",
    "\n",
    "\n",
    "            if (FEV % kratnost == 0):\n",
    "                tracking_fitness.append(global_best)\n",
    "                tracking_solution = (best_solution)\n",
    "                print(\"GLOVAL BEST, FEV: \", FEV,\": \", global_best, \", best solution: \", best_solution )\n",
    "        total_w = np.sum(w)\n",
    "        w = w/total_w\n",
    "\n",
    "        new_CR = np.sum(w*S_F)\n",
    "        new_F = np.sum(w*S_F*S_F)/np.sum(w*S_F)\n",
    "        if (new_CR >0 and new_F>0):\n",
    "            F_history[k]=new_F\n",
    "            CR_history[k]=new_CR\n",
    "            k=k+1\n",
    "            if (k>H-1):\n",
    "                k=0\n",
    "\n",
    "    print(str(RUNS))\n",
    "    RUNS = RUNS - 1\n",
    "    tracking_fitness = pd.DataFrame(tracking_fitness)\n",
    "\n",
    "    fitness_stat = pd.concat([fitness_stat,tracking_fitness], axis=1)\n",
    "    solution_stat.append(tracking_solution)\n",
    "\n",
    "    best_in_RUN.append(global_best)\n",
    "    df_stats = pd.concat([df_stats, pd.DataFrame(population)])\n",
    "    print(df_stats)\n",
    "    print(tracking_solution)\n",
    "\n",
    "\n",
    "df_stats.columns = columns\n",
    "df_stats.to_csv('XGBoost_D_ALL.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
